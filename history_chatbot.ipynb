{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team_names': 'GalaxyProject SARS-CoV-2', 'name': 'Genomics - Assembly of the genome sequence'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "import json\n",
    "\n",
    "# Recursively flatten nested metadata dictionaries/lists into key: value lines\n",
    "def flatten_metadata(obj, prefix=\"\"):\n",
    "    lines = []\n",
    "    if isinstance(obj, dict):\n",
    "        # Iterate through dictionary keys\n",
    "        for k, v in obj.items():\n",
    "            key = f\"{prefix}{k}\" if prefix else k\n",
    "            # Recursively process nested items\n",
    "            lines.extend(flatten_metadata(v, prefix=key + \".\"))\n",
    "    elif isinstance(obj, list):\n",
    "        # Enumerate list items with index\n",
    "        for i, item in enumerate(obj):\n",
    "            lines.extend(flatten_metadata(item, prefix=f\"{prefix}[{i}].\"))\n",
    "    else:\n",
    "        # Append flat key-value pair (strip trailing dot)\n",
    "        lines.append(f\"{prefix[:-1]}: {obj}\")\n",
    "    return lines\n",
    "\n",
    "# Convert a list of metadata entries into a list of  Document objects\n",
    "def convert_metadata_to_documents(metadata_list):\n",
    "    documents = []\n",
    "    for entry in metadata_list:\n",
    "        # Flatten the entire metadata entry to plain text\n",
    "        flattened_lines = flatten_metadata(entry)\n",
    "        text = \"\\n\".join(flattened_lines)\n",
    "\n",
    "        # Use @id, or fallback to url/name as document ID\n",
    "        doc_id = entry.get(\"@id\", entry.get(\"url\", entry.get(\"name\", \"unknown\")))\n",
    "\n",
    "        # Extract team names from the 'producer' field\n",
    "        doc_team_names = []\n",
    "        for producer in entry.get(\"producer\", []):\n",
    "            if isinstance(producer, dict):\n",
    "                name = producer.get(\"name\")\n",
    "                if name:\n",
    "                    doc_team_names.append(name)\n",
    "\n",
    "        # Create Document with extracted metadata and flattened content\n",
    "        documents.append(Document(\n",
    "            text=text,\n",
    "            doc_id=doc_id,\n",
    "            metadata={\n",
    "                \"team_names\":  \", \".join(doc_team_names),  # Join team names as string\n",
    "                \"name\": entry.get(\"name\", \"unknown\")       # Workflow name\n",
    "            }\n",
    "        ))\n",
    "    return documents\n",
    "\n",
    "# === Example usage ===\n",
    "with open(\"workflows-bioschemas-dump (1).jsonld\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Convert metadata list to Document list\n",
    "documents = convert_metadata_to_documents(metadata)\n",
    "\n",
    "# Print metadata of the 4th document for inspection\n",
    "print(documents[3].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5aaee",
   "metadata": {},
   "source": [
    "This project includes two alternative code implementations for building the document retrieval system. You should choose only one of them to execute, based on your use case and complexity needs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09664107",
   "metadata": {},
   "source": [
    "## Option1: MultiVectorRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import uuid\n",
    "\n",
    "# === Step 1: Create parent documents from original workflow metadata ===\n",
    "# Each parent document represents one full workflow, with a unique UUID and its metadata (name, team)\n",
    "parent_docs = []\n",
    "for doc in documents:\n",
    "    parent_id = str(uuid.uuid4())  # Generate a unique identifier for each document\n",
    "    parent_docs.append(Document(\n",
    "        page_content=doc.text,  # The full flattened metadata content\n",
    "        metadata={\n",
    "            \"doc_id\": parent_id,             # Unique ID for traceability\n",
    "            \"name\": doc.metadata[\"name\"],    # Workflow name from metadata\n",
    "            \"team\": doc.metadata[\"team_names\"]  # Contributing team names\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# === Step 2: Split parent documents into smaller chunks (child documents) ===\n",
    "# These chunks are used for vector embedding and retrieval granularity\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500,   # Each chunk has a maximum of 500 tokens\n",
    "    chunk_overlap=0   # No overlap between chunks\n",
    ")\n",
    "\n",
    "# Break each parent document into multiple child chunks, retaining doc_id for linking\n",
    "child_docs = []\n",
    "for parent in parent_docs:\n",
    "    chunks = text_splitter.split_text(parent.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        child_docs.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"doc_id\": parent.metadata[\"doc_id\"]}  # Reference back to parent\n",
    "        ))\n",
    "\n",
    "# === Step 3: Build vectorstore (Chroma) and docstore (InMemory key-value store) ===\n",
    "# Load embedding model (e.g., BAAI/bge-small-en-v1.5) for encoding document chunks\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Build Chroma vectorstore from child chunks\n",
    "vectorstore = Chroma.from_documents(\n",
    "    child_docs,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"workflow-rag\",  # Collection name for grouping\n",
    "    persist_directory=\"./chroma_store_multi_vecto_metadata_500\"  # Local persistence\n",
    ")\n",
    "\n",
    "# Persist vectorstore to disk for future use\n",
    "vectorstore.persist()\n",
    "\n",
    "# Initialize a simple in-memory docstore and populate it with parent documents\n",
    "docstore = create_kv_docstore(InMemoryStore())\n",
    "for doc in parent_docs:\n",
    "    docstore.mset([(doc.metadata[\"doc_id\"], doc)])  # Store by doc_id\n",
    "\n",
    "# === Step 4: Create MultiVectorRetriever for hybrid retrieval ===\n",
    "# Links the dense embedding retriever (child docs) with parent doc metadata\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,  # Handles similarity search on chunks\n",
    "    docstore=docstore,        # Stores and returns full parent docs\n",
    "    id_key=\"doc_id\",          # Field used to connect child and parent\n",
    "    search_kwargs={\"k\": 10}   # Retrieve top-10 most similar chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52583907",
   "metadata": {},
   "source": [
    "## Option2: vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfaf55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/912p1ty90qq72p_1fqvsx26c0000gn/T/ipykernel_56436/2709320456.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
      "/opt/anaconda3/envs/llamaenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document as LCDocument\n",
    "import json\n",
    "\n",
    "# === Step 1: Convert LlamaIndex documents into LangChain Documents ===\n",
    "# Each document is flattened and mapped to a LangChain-compatible structure\n",
    "docs_list = []\n",
    "for doc in documents:  # 'documents' is your list of LlamaIndex Document objects\n",
    "    docs_list.append(\n",
    "        LCDocument(\n",
    "            page_content=doc.text,  # Full flattened metadata text as the content\n",
    "            metadata={\n",
    "                \"source\": doc.id_,  # Unique ID from LlamaIndex Document\n",
    "                \"team\": doc.metadata.get(\"team_names\", \"\"),  # Extract team names\n",
    "                \"name\": doc.metadata.get(\"name\", \"\")         # Extract workflow name\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# === Step 2: Split long documents into chunks ===\n",
    "# Using token-based splitting (Tiktoken-compatible) for better retrieval granularity\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1500,     # Max token length per chunk\n",
    "    chunk_overlap=0      # No overlapping tokens between chunks\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)  # Returns a list of split Documents\n",
    "\n",
    "# === Step 3: Embed and store document chunks in Chroma vectorstore ===\n",
    "# Load sentence embeddings model (BAAI/bge-small-en-v1.5)\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Build a persistent Chroma vectorstore for the document chunks\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,              # Chunked documents\n",
    "    collection_name=\"rag-chroma\",      # Collection name for this index\n",
    "    embedding=embedding,               # Embedding model\n",
    "    persist_directory=\"./chroma_store\" # Directory to persist the vectorstore\n",
    ")\n",
    "\n",
    "# === Step 4: Create retriever from Chroma vectorstore ===\n",
    "# Allows downstream question-answering or RAG chains to retrieve similar documents\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc445c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"deepseek-r1:latest\", temperature=0)\n",
    "# llm = ChatOllama(model=\"gemma3:4b\", temperature=0)\n",
    "# llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "912fa570",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "def format_history(chat_history):\n",
    "    return \"\\n\\n\".join(\n",
    "        [f\"Q: {entry['question']}\\nA: {entry['answer']}\" for entry in chat_history]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# === Prompt template for rewriting follow-up questions ===\n",
    "# This prompt instructs the LLM to rewrite the user's current question into a fully self-contained version,\n",
    "# using previous chat history for context. The LLM should return only the rewritten question, without any explanation.\n",
    "rewriting_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"Your task is to rewrite the user's latest question as a self-contained question. \"\n",
    "     \"Use the chat history to understand references, but only return the rewritten question itself. \"\n",
    "     \"**Do NOT explain, do NOT include 'Let's think' or 'Thoughts' or any additional commentary. \"\n",
    "     \"Only output the final rewritten question as a plain sentence.**\"),\n",
    "    \n",
    "    # This allows injecting previous Q&A history into the prompt.\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    \n",
    "    # The latest user message (question to be rewritten).\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# === Question rewriting chain ===\n",
    "# Chain that passes the rewriting prompt to the LLM (Ollama),\n",
    "# then parses the result as plain text (string output).\n",
    "question_rewrite_chain = rewriting_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === format_history ===\n",
    "# This function takes a list of past chat history entries,\n",
    "# where each entry is a dictionary with 'question' and 'answer' fields.\n",
    "# It formats them into a readable string to be used in prompts for LLMs.\n",
    "def format_history(chat_history):\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"Q: {entry['question']}\\nA: {entry['answer']}\" for entry in chat_history\n",
    "    ])\n",
    "# === format_docs ===\n",
    "# This function takes a list of retrieved Document objects (from LangChain),\n",
    "# and returns a single string combining their metadata and content.\n",
    "# This formatted string can be passed into an LLM to provide full retrieval context.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[Metadata]: {doc.metadata}\\n[Content]: {doc.page_content}\" for doc in docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_rewritten_question(text):\n",
    "    \"\"\"\n",
    "    Clean the rewritten question returned by an LLM by removing any \n",
    "    hallucinated or unneeded <think>...</think> tags.\n",
    "\n",
    "    This is especially useful when the model fails to follow instructions\n",
    "    and outputs internal reasoning wrapped in <think>...</think> instead\n",
    "    of returning only the plain question.\n",
    "\n",
    "    Steps:\n",
    "    1. Removes anything between <think>...</think> tags (including the tags).\n",
    "    2. If only <think> is present without a closing tag, strip everything before it.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text output from the model.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned string containing only the rewritten question.\n",
    "    \"\"\"\n",
    "    # Remove complete <think>...</think> blocks if present\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Handle case where model starts with <think> but forgets to close the tag\n",
    "    if \"<think>\" in cleaned:\n",
    "        cleaned = cleaned.split(\"<think>\")[-1].strip()\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/912p1ty90qq72p_1fqvsx26c0000gn/T/ipykernel_56436/759017545.py:4: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
      "  reranker = CohereRerank(\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# Step 1: Create a reranker using Cohere's API.\n",
    "# This will re-rank retrieved chunks and select the top N most relevant ones.\n",
    "# You can replace \"rerank-v3.5\" with any other supported Cohere re-ranking model.\n",
    "reranker = CohereRerank(\n",
    "    top_n=3,  # Return the top 3 most relevant chunks\n",
    "    model=\"rerank-v3.5\",  # Use Cohere's v3.5 re-ranking model\n",
    "    cohere_api_key=\"\"  # 🔐 Replace with your own key in production!\n",
    ")\n",
    "\n",
    "# Step 2: Wrap the base retriever (multi-vector retriever) with a compression layer.\n",
    "# The ContextualCompressionRetriever first calls the base retriever (e.g. vector search),\n",
    "# then filters and re-ranks the results using the reranker above.\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker,   # Apply the Cohere reranker on top\n",
    "    base_retriever=retriever    # This is your previously defined MultiVectorRetriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main function to handle user question with context-aware rewriting and RAG answering ===\n",
    "def ask_question_with_rewriting(question, chat_history):\n",
    "    # ✅ Step 1: Rewrite the user's follow-up question into a self-contained one using chat history\n",
    "    rewritten_question = question_rewrite_chain.invoke({\n",
    "        \"chat_history\": chat_history[-3:],  # Only use the last 3 turns for rewriting (can be tuned)\n",
    "        \"input\": question\n",
    "    })\n",
    "    rewritten_question = clean_rewritten_question(rewritten_question)  # Remove unwanted tags like <think>\n",
    "    print(f\"📝 Rewritten question: {rewritten_question}\")\n",
    "\n",
    "    # ✅ Step 2: Retrieve relevant documents using the rewritten question and compression retriever (e.g., reranker)\n",
    "    docs = compression_retriever.get_relevant_documents(rewritten_question)\n",
    "    print(\"🔍 Retrieved workflow names:\", [doc.metadata.get(\"name\", \"\") for doc in docs])\n",
    "    print(\"🔍 Retrieved workflow:\", docs)\n",
    "\n",
    "    # ✅ Step 3: Build the RAG prompt using the retrieved documents\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are an assistant for question-answering tasks, and an expert in WorkflowHub. \"\n",
    "        \"Use the following pieces of retrieved context to answer the question. \"\n",
    "        \"If you don't know the answer, say that you don't know. \"\n",
    "        \"Keep the answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        \"{context}\"\n",
    "        \"\\n\\n\"\n",
    "        \"Question:\\n\"\n",
    "        \"{question}\"\n",
    "    )\n",
    "\n",
    "    # Create the final RAG chain by linking prompt → LLM → Output parser\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # ✅ Step 4: Run the full RAG chain to generate the final answer\n",
    "    response = rag_chain.invoke({\n",
    "        \"context\": format_docs(docs),        # Format retrieved docs into readable string\n",
    "        \"question\": rewritten_question,      # Pass in the rewritten question\n",
    "        # \"history\": format_history(chat_history[-3:])  # Optionally include chat history\n",
    "    })\n",
    "\n",
    "    # ✅ Step 5: Append current question and answer to chat history for next round\n",
    "    chat_history.append({\"role\": \"human\", \"content\": question})\n",
    "    chat_history.append({\"role\": \"ai\", \"content\": response})\n",
    "\n",
    "    print(\"💬 Answer:\\n\", response.strip())\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d8614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ead42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_question_with_rewriting(\"Given the workflow Genomic variants - SNPs and INDELs detection using SAMTools which other workflows are similar?\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f424b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf2ac388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Rewritten question: What are other genomic variant detection workflows similar to SNP and INDELs identification using SAMTools?\n",
      "🔍 Retrieved workflow names: ['Genomic variants - SNPs and INDELs detection using SAMTools.', 'Genomic variants - SNPs and INDELs detection using SAMTools.', 'Genomic variants - SNPs and INDELs detection using VARSCAN2.']\n",
      "🔍 Retrieved workflow : [Document(metadata={'name': 'Genomic variants - SNPs and INDELs detection using SAMTools.', 'source': 'https://workflowhub.eu/workflows/34', 'team': 'CWL workflow SARS-CoV-2', 'relevance_score': 0.8808076}, page_content='@context: https://schema.org\\n@type.[0]: SoftwareSourceCode\\n@type.[1]: ComputationalWorkflow\\ndct:conformsTo: https://bioschemas.org/profiles/ComputationalWorkflow/1.0-RELEASE/\\n@id: https://workflowhub.eu/workflows/34\\ndescription: \\r\\nAuthor: AMBARISH KUMAR er.ambarish@gmail.com; ambari73_sit@jnu.ac.in\\r\\n\\r\\nThis is a proposed standard operating procedure for genomic variant detection using SAMTools.\\r\\n\\r\\nIt is hoped to be effective and useful for getting SARS-CoV-2 genome variants.\\r\\n\\r\\n\\r\\n\\r\\nIt uses Illumina RNASEQ reads and genome sequence.\\r\\n\\nname: Genomic variants - SNPs and INDELs detection using SAMTools.\\nurl: https://workflowhub.eu/workflows/34\\nimage: https://workflowhub.eu/workflows/34/diagram?version=1\\nkeywords: CWL, SAMTools, SNPs, INDELs, covid-19\\nversion: 1\\nlicense: https://spdx.org/licenses/Apache-2.0\\nproducer.[0].@type.[0]: Project\\nproducer.[0].@type.[1]: Organization\\nproducer.[0].@id: https://workflowhub.eu/projects/6\\nproducer.[0].name: CWL workflow SARS-CoV-2\\ndateCreated: 2020-06-17T07:41:05Z\\ndateModified: 2020-06-27T01:27:06Z\\nencodingFormat: application/zip\\nprogrammingLanguage.@id: #cwl\\nprogrammingLanguage.@type: ComputerLanguage\\nprogrammingLanguage.name: Common Workflow Language\\nprogrammingLanguage.alternateName: CWL\\nprogrammingLanguage.identifier.@id: https://w3id.org/cwl/v1.0/\\nprogrammingLanguage.url.@id: https://www.commonwl.org/\\ninput.[0].@type: FormalParameter\\ninput.[0].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-inputs-sars_cov_2_reference_genome\\ninput.[0].name: sars_cov_2_reference_genome\\ninput.[0].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[1].@type: FormalParameter\\ninput.[1].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-inputs-rnaseq_left_reads\\ninput.[1].name: rnaseq_left_reads\\ninput.[1].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[2].@type: FormalParameter\\ninput.[2].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-inputs-rnaseq_right_reads\\ninput.[2].name: rnaseq_right_reads\\ninput.[2].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[3].@type: FormalParameter\\ninput.[3].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-inputs-sample_name\\ninput.[3].name: sample_name\\ninput.[3].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\noutput.[0].@type: FormalParameter\\noutput.[0].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-outputs-snp\\noutput.[0].name: snp\\noutput.[0].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\noutput.[1].@type: FormalParameter\\noutput.[1].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-outputs-indel\\noutput.[1].name: indel\\noutput.[1].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\nsdPublisher.@type: Organization\\nsdPublisher.@id: https://about.workflowhub.eu/\\nsdPublisher.name: WorkflowHub\\nsdPublisher.url: https://about.workflowhub.eu/'), Document(metadata={'source': 'https://workflowhub.eu/workflows/34', 'name': 'Genomic variants - SNPs and INDELs detection using SAMTools.', 'team': 'CWL workflow SARS-CoV-2', 'relevance_score': 0.8808076}, page_content='@context: https://schema.org\\n@type.[0]: SoftwareSourceCode\\n@type.[1]: ComputationalWorkflow\\ndct:conformsTo: https://bioschemas.org/profiles/ComputationalWorkflow/1.0-RELEASE/\\n@id: https://workflowhub.eu/workflows/34\\ndescription: \\r\\nAuthor: AMBARISH KUMAR er.ambarish@gmail.com; ambari73_sit@jnu.ac.in\\r\\n\\r\\nThis is a proposed standard operating procedure for genomic variant detection using SAMTools.\\r\\n\\r\\nIt is hoped to be effective and useful for getting SARS-CoV-2 genome variants.\\r\\n\\r\\n\\r\\n\\r\\nIt uses Illumina RNASEQ reads and genome sequence.\\r\\n\\nname: Genomic variants - SNPs and INDELs detection using SAMTools.\\nurl: https://workflowhub.eu/workflows/34\\nimage: https://workflowhub.eu/workflows/34/diagram?version=1\\nkeywords: CWL, SAMTools, SNPs, INDELs, covid-19\\nversion: 1\\nlicense: https://spdx.org/licenses/Apache-2.0\\nproducer.[0].@type.[0]: Project\\nproducer.[0].@type.[1]: Organization\\nproducer.[0].@id: https://workflowhub.eu/projects/6\\nproducer.[0].name: CWL workflow SARS-CoV-2\\ndateCreated: 2020-06-17T07:41:05Z\\ndateModified: 2020-06-27T01:27:06Z\\nencodingFormat: application/zip\\nprogrammingLanguage.@id: #cwl\\nprogrammingLanguage.@type: ComputerLanguage\\nprogrammingLanguage.name: Common Workflow Language\\nprogrammingLanguage.alternateName: CWL\\nprogrammingLanguage.identifier.@id: https://w3id.org/cwl/v1.0/\\nprogrammingLanguage.url.@id: https://www.commonwl.org/\\ninput.[0].@type: FormalParameter\\ninput.[0].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-inputs-sars_cov_2_reference_genome\\ninput.[0].name: sars_cov_2_reference_genome\\ninput.[0].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[1].@type: FormalParameter\\ninput.[1].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-inputs-rnaseq_left_reads\\ninput.[1].name: rnaseq_left_reads\\ninput.[1].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[2].@type: FormalParameter\\ninput.[2].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-inputs-rnaseq_right_reads\\ninput.[2].name: rnaseq_right_reads\\ninput.[2].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[3].@type: FormalParameter\\ninput.[3].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-inputs-sample_name\\ninput.[3].name: sample_name\\ninput.[3].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\noutput.[0].@type: FormalParameter\\noutput.[0].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-outputs-snp\\noutput.[0].name: snp\\noutput.[0].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\noutput.[1].@type: FormalParameter\\noutput.[1].@id: #genomic_variants___snps_and_indels_detection_using_samtools_-outputs-indel\\noutput.[1].name: indel\\noutput.[1].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\nsdPublisher.@type: Organization\\nsdPublisher.@id: https://about.workflowhub.eu/\\nsdPublisher.name: WorkflowHub\\nsdPublisher.url: https://about.workflowhub.eu/'), Document(metadata={'team': 'CWL workflow SARS-CoV-2', 'name': 'Genomic variants - SNPs and INDELs detection using VARSCAN2.', 'source': 'https://workflowhub.eu/workflows/31', 'relevance_score': 0.56920683}, page_content='@context: https://schema.org\\n@type.[0]: SoftwareSourceCode\\n@type.[1]: ComputationalWorkflow\\ndct:conformsTo: https://bioschemas.org/profiles/ComputationalWorkflow/1.0-RELEASE/\\n@id: https://workflowhub.eu/workflows/31\\ndescription: \\r\\nAuthor: AMBARISH KUMAR er.ambarish@gmail.com; ambari73_sit@jnu.ac.in\\r\\n\\r\\nThis is a proposed standard operating procedure for genomic variant detection using VARSCAN.\\r\\n\\r\\nIt is hoped to be effective and useful for getting SARS-CoV-2 genome variants.\\r\\n\\r\\n\\r\\n\\r\\nIt uses Illumina RNASEQ reads and genome sequence.\\r\\n\\nname: Genomic variants - SNPs and INDELs detection using VARSCAN2.\\nurl: https://workflowhub.eu/workflows/31\\nimage: https://workflowhub.eu/workflows/31/diagram?version=1\\nkeywords: CWL, SNPs, INDELs, VARSCAN2\\nversion: 1\\nlicense: https://spdx.org/licenses/Apache-2.0\\ncreator.[0].@type: Person\\ncreator.[0].@id: https://workflowhub.eu/people/26\\ncreator.[0].name: Ambarish Kumar\\nproducer.[0].@type.[0]: Project\\nproducer.[0].@type.[1]: Organization\\nproducer.[0].@id: https://workflowhub.eu/projects/6\\nproducer.[0].name: CWL workflow SARS-CoV-2\\ndateCreated: 2020-06-17T06:24:44Z\\ndateModified: 2020-06-17T06:24:44Z\\nencodingFormat: application/zip\\nprogrammingLanguage.@id: #cwl\\nprogrammingLanguage.@type: ComputerLanguage\\nprogrammingLanguage.name: Common Workflow Language\\nprogrammingLanguage.alternateName: CWL\\nprogrammingLanguage.identifier.@id: https://w3id.org/cwl/v1.0/\\nprogrammingLanguage.url.@id: https://www.commonwl.org/\\ninput.[0].@type: FormalParameter\\ninput.[0].@id: #genomic_variants___snps_and_indels_detection_using_varscan2_-inputs-sars_cov_2_reference_genome\\ninput.[0].name: sars_cov_2_reference_genome\\ninput.[0].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[1].@type: FormalParameter\\ninput.[1].@id: #genomic_variants___snps_and_indels_detection_using_varscan2_-inputs-rnaseq_left_reads\\ninput.[1].name: rnaseq_left_reads\\ninput.[1].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[2].@type: FormalParameter\\ninput.[2].@id: #genomic_variants___snps_and_indels_detection_using_varscan2_-inputs-rnaseq_right_reads\\ninput.[2].name: rnaseq_right_reads\\ninput.[2].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\ninput.[3].@type: FormalParameter\\ninput.[3].@id: #genomic_variants___snps_and_indels_detection_using_varscan2_-inputs-sample_name\\ninput.[3].name: sample_name\\ninput.[3].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\noutput.[0].@type: FormalParameter\\noutput.[0].@id: #genomic_variants___snps_and_indels_detection_using_varscan2_-outputs-snps\\noutput.[0].name: snps\\noutput.[0].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\noutput.[1].@type: FormalParameter\\noutput.[1].@id: #genomic_variants___snps_and_indels_detection_using_varscan2_-outputs-indels\\noutput.[1].name: indels\\noutput.[1].dct:conformsTo: https://bioschemas.org/profiles/FormalParameter/1.0-RELEASE/\\nsdPublisher.@type: Organization\\nsdPublisher.@id: https://about.workflowhub.eu/\\nsdPublisher.name: WorkflowHub\\nsdPublisher.url: https://about.workflowhub.eu/')]\n",
      "💬 Answer:\n",
      " <think>\n",
      "Okay, the user is asking about other genomic variant detection workflows that are similar to the one for SNPs and INDELs identification using SAMTools. Let me start by understanding what they need.\n",
      "\n",
      "First, I should look at the context provided. There's a workflow on WorkflowHub specifically named \"Genomic variants - SNPs and INDELs detection using SAMTools.\" The metadata shows it's part of the CWL SARS-CoV-2 team. Then there's another one called \"VARSCAN\" which uses VarScan2 instead of SAMTools. Both are for detecting SNPs and INDELs in SARS-CoV-2 genomes, so they're similar.\n",
      "\n",
      "The user might be a researcher or bioinformatician working with genomic data, possibly COVID-related since both workflows mention SARS-CoV-2. They want alternatives to the SAMTools workflow because maybe they need different tools for their analysis, considering factors like accuracy, speed, or compatibility with their existing pipelines.\n",
      "\n",
      "They mentioned CWL (Common Workflow Language), so they might prefer standardized, reusable workflows. The two options here are both in CWL, which is good. But perhaps there's a deeper need: the user might be looking for tools that handle different aspects of variant calling, like using other aligners or callers beyond just SAMTools and VarScan2.\n",
      "\n",
      "I should check if there are any other relevant workflows in the context provided. The given data only includes these two, so I can't mention others outside this scope. But within what's available, both use CWL and target SARS-CoV-2 variants with similar inputs (reference genome, RNA-seq reads, sample name) and outputs (SNPs and INDELs). \n",
      "\n",
      "The user might not just want a list but also an understanding of why these are alternatives. Maybe they need to know the differences between SAMTools and VarScan2 approaches. However, since I don't have detailed descriptions here, I can only point out that both use different tools but similar methodologies.\n",
      "\n",
      "Also, considering the license is Apache-2.0 for both, they might be looking for open-source options as well. The user's identity could be someone in academia or a lab needing reproducible and shareable workflows without licensing issues.\n",
      "\n",
      "In summary, the answer should list the two CWL-based workflows using SAMTools and VarScan2, explaining their similarity to the original workflow. I need to make sure not to include any other tools unless they're part of the context, which there aren't here.\n",
      "</think>\n",
      "Based on the provided context, another similar genomic variant detection workflow is:\n",
      "\n",
      "*   **Genomic variants - SNPs and INDELs detection using VARSCAN2:** This CWL-based workflow also targets SARS-CoV-2 genome variants for SNP and INDEL identification. It uses different software (VarScan2) but shares the same goal of detecting these types of variants from Illumina RNA-seq reads, similar to the SAMTools workflow.\n",
      "\n",
      "Both workflows are part of the \"CWL workflow SARS-CoV-2\" team on WorkflowHub and use CWL for standardization.\n"
     ]
    }
   ],
   "source": [
    "response = ask_question_with_rewriting(\"Given the workflow Genomic variants - SNPs and INDELs detection using SAMTools which other workflows are similar, give me the workflow?\", chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
